{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MYSSI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPqlg6/Xdkoquy07D3MdYfB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ammar999a1/Sarcasm-Detection-/blob/master/MYSSI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjtKbPnn5uYK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "efcb41e6-03e4-4c41-8beb-edb18713d181"
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "#=============================\n",
        "#       Bag-of-words\n",
        "#=============================\n",
        "\n",
        "#importing the dataset.\n",
        "dataset = 'Sarcasm_Headlines_Dataset.json'\n",
        "data_frame = pd.read_json(dataset, lines = True)\n",
        "#display(data_frame)\n",
        "\n",
        "#turning the list into string\n",
        "dataset_list = data_frame.headline.tolist()\n",
        "separator = ', '\n",
        "str_headlines = separator.join(dataset_list)\n",
        "\n",
        "#extracting words. removing stopwords and making all words lowercase.\n",
        "def word_extraction(sentence):\n",
        "    ignore = ['a', \"the\", \"is\"]\n",
        "    words = re.sub(\"[^\\w]\", \" \",  sentence).split()\n",
        "    cleaned_text = [w.lower() for w in words if w not in ignore]\n",
        "    return cleaned_text\n",
        "\n",
        "#print(word_extraction(str_headlines))\n",
        "unlem_word_list = word_extraction(str_headlines)\n",
        "\n",
        "#lemmatizing all the words\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "word_list = unlem_word_list.copy()\n",
        "for word in unlem_word_list:\n",
        "    if lemmatizer.lemmatize(word) in unlem_word_list and lemmatizer.lemmatize(word) != word:\n",
        "        word_list.remove(word)\n",
        "        word_list.append(lemmatizer.lemmatize(word))\n",
        "\n",
        "#removing duplicats and creating dictionary of each word and its frequency.\n",
        "vocab = {}\n",
        "for word in word_list:\n",
        "    if word not in vocab:\n",
        "        vocab[word] = 1\n",
        "    else:\n",
        "        vocab[word] += 1\n",
        "# print(vocab)\n",
        "# print(\"There are {} unique LEMMATIZED words in the corpus\".format(len(vocab)))\n",
        "\n",
        "#removing low frequency words.\n",
        "new_vocab = vocab.copy()\n",
        "for element in vocab:\n",
        "    if vocab[element] == 1:\n",
        "        del new_vocab[element]\n",
        "# print(new_vocab)\n",
        "# print(\"There are {} unique words in the improved corpus\".format(len(new_vocab)))\n",
        "\n",
        "#creating the zero vector.\n",
        "vector = []\n",
        "for key in vocab:\n",
        "    vector.append(0)\n",
        "#print(vector)\n",
        "\n",
        "\n",
        "#creating a vector for each headline.\n",
        "list_of_vectors = []\n",
        "for headline in data_frame.headline.tolist():\n",
        "    result = []\n",
        "    headline += ' '\n",
        "    lower_headline = headline.lower()\n",
        "    curr_word = ''\n",
        "    punc = [' ', \"'\", '\"', ',', '(', ')', '.']\n",
        "    for char in lower_headline:\n",
        "        if char not in punc:\n",
        "            curr_word += char\n",
        "        else:\n",
        "            result.append(curr_word)\n",
        "            curr_word = '' \n",
        "    for word in result:\n",
        "        temp_vector = vector.copy()\n",
        "        if word in new_vocab:\n",
        "            index = list(new_vocab).index(word)\n",
        "            if temp_vector[index] != 0:\n",
        "                temp_vector[index] += 1\n",
        "            else:\n",
        "                temp_vector[index] = 1\n",
        "    list_of_vectors.append(temp_vector)\n",
        "\n",
        "print(list_of_vectors)\n",
        "#=============================\n",
        "#       Neural Network\n",
        "#=============================\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}